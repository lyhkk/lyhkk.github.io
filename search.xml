<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BEiT V1论文笔记</title>
    <url>/2023/08/31/BEiT-V1%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p><a href="https://arxiv.org/abs/2106.08254">paper reference</a></p>
<p><a href="https://sh-tsang.medium.com/review-beit-bert-pre-training-of-image-transformers-c14a7ef7e295">note reference 1: BEiT</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/381345343">note reference 2: Self-supervised learning</a></p>
<h2 id="Self-supervised-learning"><a href="#Self-supervised-learning" class="headerlink" title="Self-supervised learning"></a>Self-supervised learning</h2><ol>
<li><strong>Overall impression</strong>: unsupervised  pretraining and supervised fine-tuning</li>
<li><strong>Goals</strong>: The objective is to acquire <strong>a set of general feature representations</strong> during <strong>the pretraining phase</strong>, which can be further refined through the utilization of numerous labeled datasets <strong>in the downstream task</strong>.</li>
<li>Masked Image Modeling (<em><strong>MIM</strong></em>) methods are proposed for <em><strong>self*-*supervised</strong></em> visual representation learning</li>
</ol>
<h2 id="BEiT-BERT-Pre-Training-of-Image-Transformers"><a href="#BEiT-BERT-Pre-Training-of-Image-Transformers" class="headerlink" title="BEiT: BERT Pre-Training of Image Transformers"></a><strong>BEiT: BERT Pre-Training of Image Transformers</strong></h2><h3 id="BEiT-Architecture"><a href="#BEiT-Architecture" class="headerlink" title="BEiT Architecture:"></a><strong>BEiT Architecture:</strong></h3><p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311756921.png" alt="image-20230824100112299"></p>
<ol>
<li><p><strong>Overall Approach</strong></p>
<ol>
<li><p>Pre-training task: <code>MIM</code>(<strong>Masked Image Modeling</strong>) -&gt; inspired by <code>BERT</code></p>
<blockquote>
<p><em>During</em> <strong>pre-training</strong>, some proportion of <strong>image patches are randomly masked</strong>, and <strong>fed the corrupted input to Transformer.</strong> <em>The model</em>  <strong>learns to recover the visual tokens</strong> <em>of the original image,</em> <strong>instead of the raw pixels</strong> of masked patches.</p>
</blockquote>
<p><code>MIM</code> uses two views of each image</p>
<ul>
<li><strong>image patches</strong></li>
<li><strong>visual tokens</strong></li>
</ul>
</li>
<li><p>The input of BEiT: The image is split into <strong>a grid of patches</strong> that are the <strong>input representation of backbone</strong> <strong>Transformer</strong></p>
</li>
<li><p>The approach to <strong>tokenize</strong> an image: <strong>latent codes of discrete VAE</strong>(VAE is from <a href="https://sh-tsang.medium.com/review-dall-e-zero-shot-text-to-image-generation-f9de7a383374"><strong>DALL</strong>·E</a>)</p>
</li>
<li><p>The goal of pretraining: reinforce the model’s capacity to capture generic visual features.</p>
</li>
</ol>
</li>
<li><p><strong>Introduction of two image representations</strong></p>
<ol>
<li><p><strong>Image patches</strong></p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311756923.png" alt="image-20230824103227012"></p>
<ol>
<li><p>The 2D image of the size H×W×C is split into <strong>a sequence of patches</strong> of size P^2^, while the number of patch is N&#x3D;HW&#x2F;P^2^ patches</p>
<blockquote>
<p>image -&gt; patches</p>
</blockquote>
</li>
<li><p>The image patches <em>xp</em> are <strong>flattened into vectors</strong> and are <strong>linearly projected</strong> which is similar to word embeddings in  BERT</p>
<blockquote>
<p>patches -&gt; patch embeddings</p>
</blockquote>
</li>
</ol>
</li>
<li><p><strong>Visual tokens</strong></p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311756924.png" alt="image-20230824103211423"></p>
<ol>
<li><p>The original image is represented as a sequence of tokens obtained by an <strong>image tokenizer</strong>, instead of raw pixels</p>
<blockquote>
<p><strong>visual tokens</strong> are <strong>discreate token indices</strong>, and the true patches can be refered respectively by <strong>the token indices and a visual codebook</strong></p>
</blockquote>
</li>
<li><p>The <strong>image tokenizer</strong> learned by discreate variational autoencoder(dVAE), by DALL·E, is directly used.</p>
<blockquote>
<p>Learning process:</p>
<p>By two modules <strong>tokenizer</strong> and <strong>decoder</strong></p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311756925.png" alt="image-20230824104526023"></p>
<p>visual codebook contains <strong>eigen vectors</strong> that represent various image patches.</p>
</blockquote>
</li>
</ol>
</li>
</ol>
</li>
<li><p><strong>ViT Backbone</strong></p>
<ol>
<li><p>Following ViT, Transformer backbone network is used</p>
<blockquote>
<p><strong>ViTBase</strong> is used, which is a 12-layer <a href="https://sh-tsang.medium.com/review-attention-is-all-you-need-transformer-96c787ecdec1">Transformer</a> with 768 hidden size, and 12 attention heads. The intermediate size of feed-forward networks is 3072.</p>
</blockquote>
</li>
<li><p>The  fomula and architecture are shown below:</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311756926.png" alt="image-20230824113010977"></p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311756927.png" alt="image-20230824112608619"></p>
</li>
</ol>
</li>
</ol>
<h3 id="BEiT-Pretraning"><a href="#BEiT-Pretraning" class="headerlink" title="BEiT Pretraning"></a><strong>BEiT Pretraning</strong></h3><ol>
<li><p><strong>MIM</strong></p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311756928.png" alt="image-20230824113304695"></p>
<ol>
<li><p>As described above, <strong>approximately 40% image patches are randomly masked</strong>, where the masked patches are denoted as M. The masked patches are <strong>replaced with a learnable embedding e[M]</strong>.</p>
</li>
<li><p>After a L-layer Transformer, <strong>a softmax classifier</strong> is used to predict corresponding visual tokens</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311756929.png" alt="image-20230824113737099"></p>
</li>
<li><p><em>pre-training objective</em>: <strong>maximize the log-likelihood of the correct visual tokens zi given the corrupted image</strong></p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311756930.png" alt="image-20230824113835611"></p>
</li>
</ol>
</li>
<li><p><strong>Blockwise Masking</strong></p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311756932.png" alt="image-20230824120716933"></p>
</li>
<li><p><strong>Why dVAE beats VAE</strong></p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311756933.png" alt="image-20230824233709250"></p>
<blockquote>
<p>VAE: directly using pixel-level auto-encoding(<strong>continuous vector space to represent latent space</strong>) -&gt; foucus on short-range dependencies and high-frequency details</p>
<p>dVAE: discrete visual tokens</p>
</blockquote>
</li>
</ol>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention Is All You Need论文笔记</title>
    <url>/2023/08/14/Attention%20Is%20All%20You%20Need%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p><a href="https://arxiv.org/abs/1706.03762">论文地址</a></p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><ol>
<li><p>Attention有无限长上下文窗口，之前序列生成算法(LSTM, GPT2)的难点</p>
</li>
<li><p>Query: 类比Google输入框</p>
<p>Key: Google根据Query为你匹配到的关键词</p>
<p>Value: 查询到的网页</p>
</li>
<li><p>Encoder-Decoder结构:</p>
<p>Encoder: 将文本序列变为带有注意力的向量表示</p>
<p>Decoder: 生成序列</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308141046791.png" alt="image-20230813110607595"></p>
</li>
<li><p>模块化解读</p>
<ol>
<li><p>Input Embedding: 输入的symbol representation映射到continuous presentation(向量)</p>
</li>
<li><p>Positional Encoding: 将token的位置信息映射到向量（位置信息对语义的表达很关键）</p>
<p>数学实现：</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308141046792.png" alt="image-20230813210127612"></p>
<p>1）d&lt;&#x3D;&gt;d<del>model</del>，和Input Embedding层输出的向量维度相同</p>
<p>​      i的计算方法由下面的示例说明</p>
<p>2）编码原则：唯一性，不同长度序列的token间间隔含义相同</p>
<p>3）为什么不用index of token作为位置编码：For long sequences, the indices can grow large in magnitude. &#x3D;&#x3D;If you normalize the index value to lie between 0 and 1, it can create problems for variable length sequences as they would be normalized differently.&#x3D;&#x3D;这违背了一个位置编码的原则：对于不同长度的序列，token的间隔的含义要相同 –&gt; 最好编码完，得到的位置信息自然∈(0,1)</p>
<p>4）Example:<img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308141046793.png" alt="image-20230813210507196"></p>
<blockquote>
<p>i的计算方法：0≤i&lt;d&#x2F;2&#x3D;2，4维的positional encoding的index&#x3D;0,1,2,3，index为奇数，2i+1&#x3D;index，index为偶数，2i&#x3D;index。上图的index对应得出i的取值0,0,1,1</p>
<p>根据示例分析是否符合编码原则：</p>
<ol>
<li>位置编码向量是否唯一：看上图矩阵第一列，编码始终为sin(2i+1)或cos(2i) (i∈N)，根据对称性，因为sin和cos对称轴始终不在自然数上，所以不可能有相同的，满足唯一性</li>
<li>不同长度的序列，token之间的间隔的含义要相同，因为这种方式产生的编码自然在(0,1)区间，不需要normalization。这里的不同长度序列，它们的(n, i, d)这个三元组取值都相同，只是k的范围不同，那么token的间隔始终相同</li>
</ol>
</blockquote>
</li>
<li><p><strong>Multi-Head Attention:</strong></p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308141046794.png" alt="image-20230813110533123"></p>
<blockquote>
<p>h for “Head” num</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308141046795.png" alt="image-20230813111743464"></p>
<p>Q, K, V都是d<del>model</del>列</p>
<p>W<del>i</del>是weight矩阵，d<del>model</del> &#x3D; n<del>head</del> * d<del>k</del></p>
</blockquote>
<ol>
<li><p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Multi-Head Attention module &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_head, d_model, d_k, d_v, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.n_head = n_head</span><br><span class="line">        self.d_k = d_k</span><br><span class="line">        self.d_v = d_v</span><br><span class="line"></span><br><span class="line">        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=<span class="literal">False</span>)</span><br><span class="line">        self.fc = nn.Linear(n_head * d_v, d_model, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.attention = ScaledDotProductAttention(temperature=d_k ** <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model, eps=<span class="number">1e-6</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span><br><span class="line"></span><br><span class="line">        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head</span><br><span class="line">        sz_b, len_q, len_k, len_v = q.size(<span class="number">0</span>), q.size(<span class="number">1</span>), k.size(<span class="number">1</span>), v.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        residual = q</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pass through the pre-attention projection: b x lq x (n*dv)</span></span><br><span class="line">        <span class="comment"># Separate different heads: b x lq x n x dv</span></span><br><span class="line">        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)</span><br><span class="line">        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)</span><br><span class="line">        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Transpose for attention dot product: b x n x lq x dv</span></span><br><span class="line">        q, k, v = q.transpose(<span class="number">1</span>, <span class="number">2</span>), k.transpose(<span class="number">1</span>, <span class="number">2</span>), v.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)   <span class="comment"># For head axis broadcasting.</span></span><br><span class="line"></span><br><span class="line">        q, attn = self.attention(q, k, v, mask=mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Transpose to move the head dimension back: b x lq x n x dv</span></span><br><span class="line">        <span class="comment"># Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)</span></span><br><span class="line">        q = q.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(sz_b, len_q, -<span class="number">1</span>)</span><br><span class="line">        q = self.dropout(self.fc(q))</span><br><span class="line">        q += residual</span><br><span class="line"></span><br><span class="line">        q = self.layer_norm(q)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> q, attn <span class="comment"># 返回输出和attention</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<p>len_q, len_k, len_v对应论文中的d<del>model</del>，并且相等</p>
</blockquote>
</li>
<li><p><strong>Scaled Dot-Product Attention:</strong></p>
<ol>
<li><p>比较Query和Key相似度-&gt;softmax-&gt;对Value加权</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308141046796.png" alt="image-20230813004534001"></p>
<blockquote>
<p>为什么要Scale: 论文原文：We suspect that for large values of d<del>k</del>, the dot products grow large in magnitude, pushing the softmax function into regions where it has <strong>extremely small gradients</strong></p>
<ol>
<li>由于softmax函数计算方法使用了指数函数，如果d<del>k</del>大了，参与QK^T^点乘的元素个数增加，产生的矩阵，各个位置的值大概率变大，反向传播时，梯度极小</li>
<li>指数函数可能存在数值的上溢和下溢问题</li>
</ol>
<p>Mask矩阵: Mask是一个bool型矩阵。在第 i 时刻做注意力计算时，&gt;i 的时刻都没有结果，只有＜i 的时刻有结果。注意力只与之前的内容相关，因此需要做Mask，将QK^T^点乘产生的矩阵中，在Mask规定的需要掩住的位置上，将结果矩阵值设为-1e9，经过softmax后就是0</p>
</blockquote>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308131104219.png" alt="image-20230813004024882"></p>
</li>
<li><p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Scaled Dot-Product Attention &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, temperature, attn_dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.temperature = temperature  <span class="comment"># 1/sqrt(d_k)</span></span><br><span class="line">        self.dropout = nn.Dropout(attn_dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span><br><span class="line"></span><br><span class="line">        attn = torch.matmul(q / self.temperature, k.transpose(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">        <span class="comment"># mask --&gt; 只能通过现有的output序列生成新的序列</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">            attn = attn.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line"></span><br><span class="line">        attn = self.dropout(F.softmax(attn, dim=-<span class="number">1</span>)) <span class="comment"># dropout应该也是option ?</span></span><br><span class="line">        output = torch.matmul(attn, v)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
</li>
<li><p>Decoder:</p>
<ol>
<li><p>搞清三个输入箭头：</p>
<ol>
<li>Outputs(shifted right): 将生成的序列作为输入</li>
<li>将Encoder输出的Attention向量作为第二个Multi-Head Attention的Attention</li>
</ol>
</li>
<li><p>为什么要用Masked Multi-Head Attention</p>
<ol>
<li><p>因为这是对生成序列进行Attention，而每一时刻生成的序列，只能与之前生成的序列关联</p>
</li>
<li><p>如何进行mask:</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308141046797.png" alt="image-20230814005832316"></p>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ STL</title>
    <url>/2023/07/05/C-STL/</url>
    <content><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ol>
<li><p>全称：Standard Template Library,标准模板库</p>
</li>
<li><p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202307061142412.png" alt="image-20230409145131333"></p>
<blockquote>
<p>迭代器是沟通算法和容器的桥梁，容器相当于数据结构，算法根据容器提供的迭代器进行操作 -&gt; 数据结构有了配套的操作</p>
</blockquote>
</li>
</ol>
<h2 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h2><p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202307061142269.png" alt="image-20230409145354236"></p>
<blockquote>
<p>容器更像一个类</p>
</blockquote>
<h3 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h3><ol>
<li><p>特点：</p>
<ol>
<li>拥有一段连续的内存空间，因此它能非常好的<code>支持随机访问</code>，即 [] 操作符和 .at()，随机访问快。（优点）</li>
<li>当向其头部或中间插入或删除元素时，为了<code>保持原本的相对次序</code>，插入或删除点之后的所有元素都必须移动，所以<code>插入或删除的效率比较低</code>。（缺点）</li>
<li>在后面插入删除元素最快，此时一般不需要移动内存。（优点）</li>
<li>总结：相当于可拓展的数组（<code>动态数组</code>），随机访问快，在头部和中间插入或删除效率低，但在尾部插入或删除效率高。</li>
</ol>
</li>
<li><p>操作：</p>
<p>1.<code>push_back</code> 在数组的最后添加一个数据</p>
<p>2.pop_back 去掉数组的最后一个数据</p>
<p>3.<code>at</code> 得到编号位置的数据</p>
<p>4.begin 得到数组头的指针</p>
<p>5.end 得到数组的最后一个单元+1的指针</p>
<p>6.front 得到数组头的引用</p>
<p>7.back 得到数组的最后一个单元的引用</p>
<p>8.max_size 得到vector最大可以是多大</p>
<p>9.capacity 当前vector分配的大小</p>
<p>10.<code>size</code> 当前使用数据的大小</p>
<p>11.resize 改变当前使用数据的大小，如果它比当前使用的大，者填充默认值</p>
<p>12.reserve 改变当前vecotr所分配空间的大小</p>
<p>13.erase 删除指针指向的数据项</p>
<p>14.<code>clear</code> 清空当前的vector</p>
<p>15.rbegin 将vector反转后的开始指针返回(其实就是原来的end-1)</p>
<p>16.rend 将vector反转构的结束指针返回(其实就是原来的begin-1)</p>
<p>17.empty 判断vector是否为空</p>
<p>18.swap 与另一个vector交换数据</p>
</li>
<li><p>特性：</p>
<ol>
<li>在动态扩容上，vector动态增加大小时，并不是在原空间之后持续新空间（因为无法保证原空间之后尚有可供配置的空间），而是<code>以原大小的两倍另外配置一块较大的空间，然后将原内容拷贝过来</code>，然后才开始在原内容之后构造新元素，并释放原空间。因此， 对vector的任何操作，<code>一旦引起空间重新配置，指向原vector的所有迭代器就都失效了</code>。</li>
</ol>
</li>
</ol>
<h3 id="deque"><a href="#deque" class="headerlink" title="deque"></a>deque</h3><ol>
<li><p>deque（double-ended queue）是双向开口的连续内存空间（动态将多个连续空间通过指针数组接合在一起），随时可以增加一段新的空间。deque 的最大任务就是<code>在这些分段的连续空间上，维护其整体连续的假象，并提供随机存取的接口</code>。</p>
</li>
<li><p><strong>特点</strong></p>
<ul>
<li><p>一旦要在 deque 的头部和尾部增加新空间，便配置一段定量连续空间，串在整个 deque 的头部或尾部，因此不论在头部或尾部插入元素都十分迅速。 (优点）</p>
</li>
<li><p>在中间部分安插元素则比较费时，因为必须移动其它元素。（缺点）</p>
</li>
<li><p>deque 是 list 和 vector 的折中方案。兼有 list 的优点，也有 vector 随机访问效率高的优点。</p>
</li>
<li><p>总结：支持随机访问，但效率没有 vector 高，在头部和尾部插入或删除效率高，但在中间插入或删除效率低。</p>
</li>
</ul>
</li>
</ol>
<h3 id="set"><a href="#set" class="headerlink" title="set"></a>set</h3><ol>
<li>每个元素最多只出现一次，并且 set 中的元素已经从小到大排好序</li>
<li><strong>特点</strong><ul>
<li>使用红黑树实现，其内部元素依据其值自动排序，每个元素值只能出现一次，不允许重复。</li>
<li>每次插入值的时候，都需要调整红黑树，效率有一定影响。（缺点）</li>
<li>map 和 set 的插入或删除效率比用其他序列容器高，因为对于关联容器来说，不需要做内存拷贝和内存移动。（优点）</li>
<li>总结：由红黑树实现，其内部元素依据其值自动排序，每个元素值只能出现一次，不允许重复，且插入和删除效率比用其他序列容器高。</li>
</ul>
</li>
</ol>
<h3 id="list"><a href="#list" class="headerlink" title="list"></a>list</h3><p>List 由双向链表（doubly linked list）实现而成，元素<u>存放在堆中</u>，每个元素都是放在一块内存中。没有空间预留习惯，所以<u>每分配一个元素都会从内存中分配，每删除一个元素都会释放它占用的内存</u>。</p>
<p><strong>特点</strong></p>
<ul>
<li>内存空间可以是不连续的，通过指针来进行数据的访问，这个特点使得它的随机存取变得非常没有效率，因此它没有提供 [] 操作符的重载。（缺点）</li>
<li>由于链表的特点，在任意位置的插入和删除效率都较高。（优点）</li>
<li><u>只支持首尾两个元素的直接存取</u>，想获取其他元素（访问时间一样），则需要遍历链表。（缺点）</li>
<li>总结：不支持随机访问，<u>在任意位置的插入和删除效率都较高</u>。</li>
</ul>
<h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p>map 由红黑树实现，其元素都是 <code>“键值/实值”</code>所形成的一个对组（key&#x2F;value pairs)。</p>
<p>map 主要用于资料一对一映射的情况，map 内部自建一颗红黑树，这颗树具有对数据自动排序的功能，所以在 map 内部所有的数据都是有序的。比如一个班级中，每个学生的学号跟他的姓名就存在着一对一映射的关系。</p>
<p><strong>特点</strong></p>
<ul>
<li>每个元素都有一个键，且只能出现一次，不允许重复。</li>
<li>根据 key 值快速查找记录，查找的复杂度基本是 O(logN)，如果有 1000 个记录，二分查找最多查找 10次(1024)。（优点）</li>
<li>每次插入值的时候，都需要调整红黑树，效率有一定影响。（缺点）</li>
<li>增加和删除节点对迭代器的影响很小，除了那个操作节点，对其他的节点都没有什么影响。（优点）</li>
<li>对于迭代器来说，可以修改实值，而不能修改 key。</li>
<li>总结：元素为键值对，key 和 value 可以是<code>任意你需要的类型</code>，每个元素都有一个键，且只能出现一次，不允许重复，根据 key 快速查找记录。</li>
</ul>
<p><strong>适用场景</strong></p>
<p>适用于需要存储一个数据字典，并要求方便地根据key找value的场景。</p>
<h2 id="iterator"><a href="#iterator" class="headerlink" title="iterator"></a>iterator</h2><ol>
<li><p>迭代器类似指针，指向容器中的某个元素，用-&gt;(如果容器中的元素是pair，可以用it-&gt;first与it-&gt;second访问第一个和第二个值)或者*(元素为一个值)访问指向的元素</p>
</li>
<li><p>容器适配器 stack、queue 和 priority_queue <code>没有</code>迭代器。容器适配器有一些成员函数，可以用来对元素进行访问。</p>
</li>
<li><p>分类：</p>
<ol>
<li>正向：容器类名&lt;&gt;::iterator 迭代器名;</li>
<li>反向：容器类名&lt;&gt;::reverse_iterator 迭代器名;</li>
<li>常量正向：容器类名&lt;&gt;::const_iterator 迭代器名;</li>
<li>常量反向：容器类名&lt;&gt;::const_reverse_iterator 迭代器名;</li>
</ol>
</li>
<li><p>迭代器都可以进行<code>++</code>操作。反向迭代器和正向迭代器的区别在于：</p>
<ul>
<li><p>对正向迭代器进行<code>++</code>操作时，迭代器会指向容器中的后一个元素；</p>
</li>
<li><p>而对反向迭代器进行<code>++</code>操作时，迭代器会指向容器中的前一个元素。</p>
<blockquote>
<p>使用正向&#x2F;反向迭代器写for循环迭代时，要写it !&#x3D; container.end()&#x2F;rend()而不是&lt;&#x3D;，因为反向迭代器的++是反向的，都这么写美观整齐</p>
</blockquote>
</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Language</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>BEiT V2论文笔记</title>
    <url>/2023/08/31/BEiT-V2%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p><a href="https://arxiv.org/abs/2208.06366">BEiT V2 paper</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/566511151">paper note reference</a></p>
<h2 id="BEiT-V2-architecture"><a href="#BEiT-V2-architecture" class="headerlink" title="BEiT V2 architecture"></a>BEiT V2 architecture</h2><ol>
<li><p><strong>Overall impression</strong>: two-stage training </p>
<ol>
<li>input image -&gt; visual tokens -&gt; visual presentation (approach: <strong>VQ-KD</strong>)</li>
<li>masked image -&gt;  visual presentation from stage 1 (approach: <strong>MIM</strong>)</li>
</ol>
</li>
<li><p><strong>VQ-KD process</strong>（Vector-quantized Knowledge Distillation）</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311057051.png" alt="image-20230825105555838"></p>
<ol>
<li><p><strong>Visual</strong> <strong>codebook</strong>: For a given dataset of visual features extracted from images, we can employ the <strong>K-means algorithm</strong> to <strong>cluster</strong> them into <strong>several prominent categories</strong>. The number of these categories determines the size of the visual codebook, where <strong>each category’s centroid</strong> represents <strong>the specific content of a visual word in the dictionary.</strong></p>
</li>
<li><p><strong>Tokenizer encoder</strong></p>
<ol>
<li><p><strong>ViT</strong>: encode the image patches and generate <strong>eigen vector representations</strong>(From the above figure, N for patch number, D for the dimension of vector)</p>
</li>
<li><p><strong>Nearest Neighbor Lookup</strong>: employ the <strong>L2-norm</strong> to regularize encoder output {h<del>i</del>^p^}<del>i&#x3D;1</del>^N^ and codebook embeddings {e<del>i</del>^p^}<del>i&#x3D;1</del>^K^ , then cauculate the <strong>minimum cosine distance</strong> between them.</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311057052.png" alt="image-20230825112433459"></p>
</li>
</ol>
</li>
<li><p><strong>Decoder</strong></p>
<p>Using the nearest neighbor of each image patch as the input of <em>Decoder</em> part. Output is the <strong>semantic reconstruction</strong> of these visual tokens.</p>
</li>
<li><p><strong>Optimization target(teacher system)</strong></p>
<p>The strategy of VQ-KD involves <strong>utilizing the feature learning strategies proposed in model distillation</strong>. VQ-KD employs <em><strong>CLIP</strong></em>  or <em><strong>DINO</strong></em>  as the <strong>teacher</strong> systems, and utilizes <strong>the features generated by the teacher systems</strong> as the <strong>optimization objective</strong> for training the model.</p>
</li>
<li><p><strong>Gradient backpropagation</strong></p>
<p>The <em>arg min</em> function is <strong>non-differentiable</strong>. In order to backpropagate gradients to the encoder, VQ-KD adopts the approach proposed in VQ-VAE which <strong>directly copies the gradients from the decoder’s input to the encoder’s output</strong> (indicated by <strong>the red arrow in Figure 1</strong>), as their <strong>optimization directions align</strong>.</p>
</li>
</ol>
</li>
<li><p><strong>BEiT V2 pretrain(MIM)</strong> -&gt; copy from <a href="https://zhuanlan.zhihu.com/p/566511151">blog</a></p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311057054.png" alt="image-20230825113940448"></p>
<ol>
<li><p><strong>MIM</strong></p>
<ol>
<li><p><code>[CLS]</code> token is settled to learn global information</p>
</li>
<li><p>The left part of <code>Figure 3</code> is MIM pretraining process.</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311057055.png" alt="image-20230825114829154"></p>
</li>
</ol>
</li>
<li><p><code>[CLS]</code> <strong>pretrain</strong></p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308311057056.png" alt="image-20230825121059114"></p>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Swin Transformer V2论文笔记</title>
    <url>/2023/08/20/Swin-Transformer-V2%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p><a href="https://sh-tsang.medium.com/review-swin-transformer-v2-scaling-up-capacity-and-resolution-401c28b02df8">Referance blog website</a></p>
<p><a href="https://arxiv.org/abs/2111.09883">paper</a></p>
<h2 id="V1-V2模型对比："><a href="#V1-V2模型对比：" class="headerlink" title="V1 V2模型对比："></a>V1 V2模型对比：</h2><p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308200346006.png" alt="image-20230818172517101"></p>
<ol>
<li><p>Residual-post-norm Method</p>
<ol>
<li><p>好处：增强训练的稳定性</p>
</li>
<li><p>Swin V2中的residual-post-norm</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308200346007.png" alt="image-20230818172747611"></p>
<blockquote>
<p>训练更稳定的实验数据依托：</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308200346008.png" alt="image-20230818173101101"></p>
<p>Pre方法：Swin V1使用的normalization策略</p>
<p>Post方法：Swin V2使用的normalization策略</p>
<hr>
<p>Figure 2应该从两个角度看：</p>
<ol>
<li>先看Pre方法下(节点用’●’表示)，随着模型规模增大，层数增加，输出值不断累加，深层输出和浅层输出幅值差很多，导致训练过程不稳定</li>
<li>再看同一数据规模下(同一颜色的线条)，Pre方法的activation amplitudes更小，故训练更稳定</li>
</ol>
</blockquote>
</li>
</ol>
</li>
<li><p>Scaled Cosine Attention</p>
<ol>
<li><p>使用点积的方法计算相关性，一些blocks &amp; heads的attention map会受到小部分的像素对的支配，这个影响在使用residual-post-norm时更为显著（为什么？）</p>
</li>
<li><p>计算pixel i和j之间的相关性：</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308200346009.png" alt="image-20230818175331644"></p>
<blockquote>
<p>优势：milder attention value, naturally normalized function</p>
</blockquote>
</li>
</ol>
</li>
<li><p>Scaling Up Window Resolution</p>
<ol>
<li><p>原始的方法：相对位置编码是[-M+1, M-1]，当跨不同窗口大小进行<strong>迁移学习</strong>时，预训练中学习到的相对位置偏差矩阵用于通过<strong>双三次插值</strong>来初始化微调时的偏差矩阵(windw size不同)。</p>
</li>
<li><p>CPB：通过两层MLP，取代双三次插值，因为用到了可学习的网络，更灵活，用于迁移学习更优</p>
</li>
<li><p>Log-Spaced CPB(CPB for Continous Positional Bias)</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308200346010.png" alt="image-20230818184757407"></p>
<blockquote>
<p>计算公式：<img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308200346011.png" alt="image-20230818185012606"></p>
<p>sign()是指出正负号的函数</p>
<p>eg: 当从8×8 window size的预训练模型迁移到16×16 window size时，输入的相对位置偏移量的区间(input coordinate range)从[-7, 7] -&gt; [-15, 15] (V1不使用CPB)</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>extrapolation ratio(扩率)</td>
</tr>
<tr>
<td>不使用CPB</td>
<td>(15-7)&#x2F;7 &#x3D; <strong>1.14</strong></td>
</tr>
<tr>
<td>Log-Spaced CPB</td>
<td>ln(7+1)&#x3D;2.079, ln(15+1)&#x3D;2.773   (2.773-2.079)&#x2F;2.079 &#x3D; <strong>0.33</strong></td>
</tr>
</tbody></table>
<p>总结：外扩率减小，Meta network(MLP)的输入维持在比较稳定的区间，保证最后产生的相对位置编码的准确性</p>
</blockquote>
</li>
</ol>
</li>
<li><p>SimMIM进行自监督训练（目前不了解）</p>
</li>
<li><p>Ways to save GPU memory</p>
<ol>
<li>Zero-Redundancy Optimizer: 将模型参数和相应的optimization states分布到不同GPU上，<em>significantly</em> 节约显存开销</li>
<li>Activation Check-Pointing: 减少Feature maps存储的开销，但会增长30%训练时间</li>
<li>Sequential Self-Attention Computation: 取代了基于batch的self-attention预算。这个优化应用在前两个stage，减少显存开销，对训练时间影响小</li>
</ol>
</li>
<li><p>model variant</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308200346012.png" alt="image-20230818180914168"></p>
</li>
</ol>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>MetaRL</title>
    <url>/2023/08/29/MetaRL/</url>
    <content><![CDATA[<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308291726051.png" alt="image-20230829104324912"></p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308291726052.png" alt="image-20230829104553767"></p>
<blockquote>
<p>Episode: 一对Within-Task Traing &amp; Testing</p>
</blockquote>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308291726053.png" alt="image-20230829105012845"></p>
<blockquote>
<p>Inner Loop in “Learning to initialize” paper &lt;&#x3D;&gt; Within-task Training</p>
<p>Outer Loop in “Learning to initialize” paper &lt;&#x3D;&gt; Across-task Training</p>
</blockquote>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308291726054.png" alt="image-20230829110412310"></p>
<blockquote>
<p>Development task: 类似于验证集，用来调整Meta Learning使用的超参数，目前很多论文都没有这个task</p>
</blockquote>
]]></content>
      <categories>
        <category>Reinforcement Learning</category>
      </categories>
      <tags>
        <tag>Meta Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>ViT Adapter论文笔记</title>
    <url>/2023/08/29/ViT-Adapter%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="ViT-Adapter网络结构学习"><a href="#ViT-Adapter网络结构学习" class="headerlink" title="ViT Adapter网络结构学习"></a>ViT Adapter网络结构学习</h2><p><a href="https://arxiv.org/abs/2205.08534">Paper reference</a></p>
<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><ol>
<li><p>本论文提出的ViT Adapter是用在<code>Dense prediction task</code>上的，包括语义分割、实例分割等像素级预测的任务</p>
</li>
<li><p>与Plain ViT的对比：</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308291726288.png" alt="image-20230829152421183"></p>
<blockquote>
<p>对比plain ViT，使用多模态的数据集预训练backbone，应用到下游任务时，选用合适的Adapter去微调</p>
</blockquote>
</li>
</ol>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308291726290.png" alt="image-20230829161300843"></p>
<blockquote>
<p>(a) ViT: 相当于没有MLP Head的Plain ViT</p>
<p>(b) ViT-Adapter: 包括(c) (d) (e)三个部分，主要职责是产生多尺度的特征表示，解决了Plain ViT产生单尺度、低分辨率特征图的问题，更适合Dense prediction task</p>
<p>(c) Spatial Prior Module:</p>
<ol>
<li><p>model local spatial contexts from the input image(对原始输入图像的空间上下文信息建模)</p>
</li>
<li><p>这个module和ViT的<code>Patch Embedding</code>是平行的，先通过<code>stem</code>(3个卷积层和一个最大池化层组成，其中，第一个卷积层使得图像分辨率&#x2F;2, 通道数由3变为inplane；最大池化层让分辨率&#x2F;2)，再通过3个卷积层(每个都是分辨率&#x2F;2，通道数×2) -&gt; 产生了1&#x2F;8, 1&#x2F;16, 1&#x2F;32三个分辨率的特征图，再将3个尺度<code>1/8, 1/16, 1/32</code>的特征图映射到D-dimension，flatten(特征图-&gt;向量) &amp; concat -&gt; <code>D-dimensional spatial feature token</code>(如下图F<sub>sp</sub>^1^)</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308291726291.png" alt="image-20230829163954662"></p>
</li>
<li><p>输出：具有多尺度的空间特征</p>
</li>
</ol>
<p>(d) Spatial Feature Injector: (F<sub>vit</sub><sup>i</sup>代表即将输入到ViT的Block i的特征)</p>
<ol>
<li><p>顾名思义，将先验的空间特征”注射”给ViT的第i个block的输入，通过<code>Cross-Attention</code>机制交互-&gt;获得多尺度空间信息</p>
</li>
<li><p>数学表示：<img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308291726292.png" alt="image-20230829164218239"></p>
<p>假设输入分patch时分为16×16个patch</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308291726293.png" alt="image-20230829164253193"></p>
<p>γ是可学习向量∈R<sup>D</sup>，用来平衡ViT第 i block的输入和<code>Cross-Attention</code>之后的输出</p>
</li>
</ol>
<p>(e) Multi-Scale Feature Extractor:</p>
<ol>
<li><p>基于ViT第i个Blcok输出的单尺度特征(记为F<sub>vit</sub><sup>i+1</sup>)，重组(reorganize)多尺度的空间特征(F<sub>sp</sub><sup>i</sup> -&gt; F<sub>sp</sub><sup>i+1</sup>)</p>
</li>
<li><p>数学表示：</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308291726294.png" alt="image-20230829170356182"></p>
<blockquote>
<p>与Injector不同的是，进入<code>Cross-Attention</code>的query和(key,value)的角色互换了，因为Injector是依据F<sub>sp</sub><sup>i</sup> (作为key和value)产生F<sub>vit</sub><sup>i</sup>(query)，Extractor是依据F<sub>vit</sub><sup>i+1</sup>(key,value)产生F<sub>sp</sub><sup>i+1</sup>(F<sub>sp</sub><sup>i</sup>为query)</p>
</blockquote>
</li>
<li><p>FFN是Feed-Forward Network(前馈神经网络)，实现ViT Adapter多用<code>Conv FFN</code>，增强非线性表达能力(Attention是矩阵乘法，是线性变化)</p>
</li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Swin Transformer V1论文笔记</title>
    <url>/2023/08/16/Swin-Transformer-V1%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p><a href="https://arxiv.org/abs/2103.14030">paper</a></p>
<h2 id="Swin-Transformer-V1网络结构："><a href="#Swin-Transformer-V1网络结构：" class="headerlink" title="Swin-Transformer-V1网络结构："></a><strong>Swin-Transformer-V1网络结构：</strong></h2><p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308161147454.png" alt="image-20230814090947639"></p>
<h3 id="不同参数量级的Swin的设置："><a href="#不同参数量级的Swin的设置：" class="headerlink" title="不同参数量级的Swin的设置："></a><strong>不同参数量级的Swin的设置：</strong></h3><p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308161147455.png" alt="image-20230814095932385"></p>
<h3 id="网络结构解读："><a href="#网络结构解读：" class="headerlink" title="网络结构解读："></a><strong>网络结构解读：</strong></h3><ol>
<li><p>Patch Partition: 为每一个图片分为不同patch</p>
</li>
<li><p>Linear Embedding: 用于Stage 1: 将通道数48映射到任意的C，很像序列attention的第一步：将符号信息-&gt;向量</p>
</li>
<li><p>Patch merging: 降采样，比如stage2: merging的作法是，将feature map切patch，为[H&#x2F;8, W&#x2F;8]，channel维变为4C，[B, H&#x2F;8, W&#x2F;8, 4C]再通过一个线性层将4C映射到2C</p>
</li>
<li><p>Window partition: 将<code>(B,H,W,C)</code>的图片划分为<code>(num_windows*B, window_size, window_size, C)</code></p>
</li>
<li><table>
<thead>
<tr>
<th align="center"></th>
<th align="center">Window Partition</th>
<th align="center">computation skills</th>
<th align="center">context</th>
</tr>
</thead>
<tbody><tr>
<td align="center">W-MSA</td>
<td align="center">regular</td>
<td align="center">–</td>
<td align="center">only window</td>
</tr>
<tr>
<td align="center">Sw-MSA</td>
<td align="center">shifted</td>
<td align="center">cyclic shift</td>
<td align="center">only window</td>
</tr>
<tr>
<td align="center">MSA</td>
<td align="center">regular</td>
<td align="center">–</td>
<td align="center">global</td>
</tr>
</tbody></table>
<blockquote>
<p>Layer l - regular partition                                   Layer l+1 - shifted partition</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308161147457.png" alt="image-20230814092813118"></p>
<p>shifted window <em>Pros</em>   克服了窗口间缺乏connections的弱点</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308161147458.png" alt="image-20230814093001309"></p>
<p>无计算技巧时的shifted window计算量：</p>
<p>以Figure 2为例：regular计算量是2×2的窗口数。而shifted如果不用cyclic技巧，就需要加Pad，将所有窗口都变为4×4Patch的窗口大小，再计算，所以计算量就是3×3的窗口数，计算量×2.25</p>
<p><code>Cyclic shift computation skill</code>: 只用4 windows的计算量，得到9 windows的self-attention结果</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308161147459.png" alt="image-20230814094808828"></p>
<p>这样的话计算量还是2×2的窗口数，而且加入了窗口间的connection</p>
<p><code>masked MSA</code>: 先明确<code>window-based self-attention</code>机制是窗口内部j计算attention，按照<code>shifted window partition</code>的分割方法，例：A模块即使被<code>cyclic shift</code>到图像的下方，也只能和A的转置进行计算attention，不然如果A是天空，灰色部分是地面，shift后A矩阵和灰色部分的矩阵的点乘得到的结果是不合理的，故计算的时候需要<code>mask</code>，负责遮住A与非A部分的矩阵点乘结果。这也符合我们对<code>computation skill</code>的理解，加入了<code>cyclic shift</code>的<code>Sw-MSA</code>与<code>有pad的W-MSA</code>计算的attention结果相同</p>
<p><code>reverse cyclic shift</code>: 将A,B,C复位</p>
</blockquote>
</li>
<li><p>Relative position bias</p>
<ol>
<li><p>修改后的Attention(Q,K,V)</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308161147460.png" alt="image-20230816110225146"></p>
<blockquote>
<p>B是[M<sup>2</sup>, M<sup>2</sup>]大小的矩阵</p>
</blockquote>
</li>
<li><p>给一个Window的不同位置的patch加一个位置编码（<a href="https://zhuanlan.zhihu.com/p/507105020">reference blog</a>）</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308161147461.png" alt="image-20230816110001915"></p>
<blockquote>
<p>拉成1维合并后 + M-1 -&gt; 索引非负</p>
<p>行标×(2M-1) 可以区分开关于主对角线对称的位置，并且对结果加和，区间为[0,8]，这个结果可以作为<code>relative_position_bias_table</code>的索引</p>
<p>table怎么计算：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">relative_position_bias_table = nn.Parameter(torch.zeros((<span class="number">2</span> * window_size[<span class="number">0</span>] - <span class="number">1</span>) * (<span class="number">2</span> * window_size[<span class="number">1</span>] - <span class="number">1</span>)))</span><br><span class="line">trunc_normal_(relative_position_bias_table, std=<span class="number">.02</span>) <span class="comment"># 正态分布应该是为了控制相对位置编码在(-1, 1)之间，位置编码的原则可以看我Attention is all you need的论文笔记</span></span><br></pre></td></tr></table></figure>

</blockquote>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>跑通Swin Transformer V2语义分割任务</title>
    <url>/2023/08/15/%E8%B7%91%E9%80%9ASwin-Transformer-V2%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<p><a href="https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation"><strong>github仓库地址</strong></a></p>
<p><a href="https://github.com/ChristophReich1996/Swin-Transformer-V2">Swin Transformer V2的实现</a></p>
<p><a href="https://lyhkk12138.cn/2023/08/16/Swin-Transformer-V1%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">Swin Transformer V1博文地址</a></p>
<p><strong>Configure environment</strong></p>
<p>使用V1的环境，将Swin V2的backbone的实现放入对应conda虚拟环境的<code>python/site-packages/mmseg/model/backbone</code>之中，并且要在<code>Class SwinTransformerV2</code>前面添加两行代码，这是关于<code>mmseg</code>的<code>Registry</code>类，只有将V2注册到<code>BACKBONES</code>之中，才能使用<code>Swin V2</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> ..builder <span class="keyword">import</span> BACKBONES</span><br><span class="line"><span class="meta">@BACKBONES.register_module()</span></span><br></pre></td></tr></table></figure>

<p><strong>制作自己的config文件</strong></p>
<p>类比于<code>Swin V1+upernet</code>的<code>config文件</code>，但目前没有调好，主要要改变的是关于模型的部分，但目前还没有调好，其中，比较重要的是<code>window size</code>和<code>input resolution</code>两个超参，目前我使用的是<code>window size=7，input resolution=[224,224]</code>，能跑但效果不佳</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>Semantic Segmentation</tag>
        <tag>Configure Environment</tag>
      </tags>
  </entry>
  <entry>
    <title>跑通Swin Transformer V1语义分割任务</title>
    <url>/2023/08/10/%E8%B7%91%E9%80%9ASwin-Transformer-V1%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<p><a href="https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation"><strong>github仓库地址</strong></a></p>
<p><a href="https://lyhkk12138.cn/2023/08/15/%E8%B7%91%E9%80%9ASwin-Transformer-V2%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/">Swin-Transformer-V2 blog</a></p>
<p><a href="https://github.com/microsoft/Swin-Transformer/blob/main/MODELHUB.md">Swin-Transformer pretrain models</a></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a><strong>简介</strong></h2><ol>
<li>代码是<code>openmmlab</code>用<code>mmcv库</code>和<code>mmseg toolbox</code>实现了训练，验证，测试过程，支持多种语义分割模型，本次实验采用Swin-Transformer为backbone的模型</li>
<li>实验难点：<code>mmcv</code>和<code>mmseg</code>的安装，环境配置</li>
<li>硬件环境：腾讯云租用<code>V100 GPU服务器</code>(特价)，32G显存，10核</li>
</ol>
<h2 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h2><p><strong>2023.8.8：</strong>先按照现有环境(cuda&#x3D;11.8.0, torch&#x3D;2.0.1)直接装mmcv和mmseg试试看</p>
<blockquote>
<p>目前环境：</p>
<p>mmseg&#x3D;1.1.1<br>mmcv&#x3D;2.0.1<br>mmengine&#x3D;0.8.4(mmengine为mmcv大改之后新增的库，支持很多原来放在mmcv里的接口，比方说train的代码，data_parallel的内容)</p>
<p>官网提供的版本依赖关系：<img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308102241754.png" alt="image-20230808150904656"></p>
<p>根据官网文档，mmcv和mmseg匹配，但是<code>module not found</code>，因为<code>train</code>的部分已经从mmcv移除</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308102241756.png" alt="image-20230808151010143"></p>
</blockquote>
<p><strong>2023.8.9：</strong></p>
<blockquote>
<p>找到的适配环境：</p>
<p>mmseg&#x3D;0.11.0</p>
<p>mmcv&#x3D;1.3.0</p>
<p>torch&#x3D;1.7.0</p>
<p>cuda&#x3D;11.0</p>
</blockquote>
<p>如何找到的环境：</p>
<ol>
<li><p>requirements.txt没给出mmcv和mmseg的版本，但源码有mmseg文件夹，进入<code>mmseg/version.py</code>可以得知mmseg需要0.11.0版本，并且需要mmcv满足<code>1.0.4-1.3.0</code>之间</p>
<p>-&gt;mmcv&#x3D;1.3.0，mmseg&#x3D;0.11.0。</p>
</li>
<li><p>因为mmcv包括了一些cuda运算的内容，<strong>mmcv必须与torch、cuda版本匹配，同时也有不支持的python版本</strong></p>
<ol>
<li><p>发现一个html网页(格式为: </p>
<p> downloadopenmmlab.com&#x2F;mmcv&#x2F;dist&#x2F;cu<xxx>&#x2F;torch&lt;x.x.x&gt;&#x2F;index.html，其中xxx和x.x.x代表版本号)可以查找，特定torch和cuda版本下的mmcv的.whl包。</p>
</li>
<li><p>尝试不同版本号，最终找到<a href="https://download.openmmlab.com/mmcv/dist/cu110/torch1.7.0/index.html">合适的版本</a>为：cuda&#x3D;11.0，torch&#x3D;1.7.0，python&#x3D;3.8。下载.whl包</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308102241757.png" alt="image-20230810221836793"></p>
</li>
<li><p>重装系统–cuda&#x3D;11.0，再安装mmcv和mmseg。</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308102241758.png" alt="image-20230810221728646"></p>
</li>
</ol>
</li>
<li><p>安装mmseg: 但不管是pip和open-mmlab提供的mim包，都找不到mmseg&#x3D;0.11.0(太旧)，所以直接把github库给的源码，复制到对应虚拟环境python3.8的packages </p>
</li>
<li><p>安装mmcv: mmcv有mmcv-full, mmcv, mmcv-lite三种，直接pip或mim安装的mmcv包缺少mmseg需要的接口，需要full版本，使用下载的mmcv-full 1.3.0的.whl包，pip安装即可。（好像直接安装mmcv-full也行，不用找.whl包）</p>
</li>
<li><p>PS: 因为我是单GPU训练，它的config文件(configs&#x2F;_base&#x2F;models&#x2F;upernet_swin.py)中有一个地方接口没对好，使用了<code>SyncBN</code>，这个需要手动改为<code>BN</code>，改正就可以跑动了。</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308102241759.png" alt="image-20230810222549116"></p>
</li>
</ol>
<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p><strong>2023.8.9:训练Swin-B模型</strong></p>
<p>对标的结果：</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308102241760.png" alt="image-20230810224052923"></p>
<p>开始训练，只加载了预训练模型(github代码页提供)，改动了batch size</p>
<p>(configs&#x2F;swin&#x2F;upernet_swin_base_patch4_window7_512x512_160k_ade20k.py)中的data段改为<strong>samples_per_gpu&#x3D;8</strong>(因为github是8GPUs分布式训练，我只有单GPU，并且开不到16，&#x3D;8时已经占用24G显存)</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308102241761.png" alt="image-20230810222731182"></p>
<blockquote>
<p>本实验的batch size影响很大，当我samples_per_gpu设为4时，对比作者的log文件，同样15950iterations下</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308102241762.png" alt="image-20230810223522211"></p>
</blockquote>
<p><strong>2023.8.10:</strong></p>
<p>跑的好慢！！！应该去跑Swin-S的，参数量少，应该速度会快很多</p>
<p>北京时间22:45，<strong>跑到了96k iterations</strong>的checkpoint(每16k iterations一次验证), validation结果和github在96k iterations的checkpoint处结果对标：</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308102315467.png" alt="image-20230810225538462"></p>
<p><strong>2023.8.11:</strong></p>
<p>三天<strong>跑完160k iterations</strong>, 最终指标与github对比：(val mAcc与96k iterations对比，不升反降，openmmlab提供的log文件也有这个现象)</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308140903172.png" alt="image-20230812004324648"></p>
<p><strong>附录：</strong></p>
<p>validation过程中，各类别的mIoU和Acc如下：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">+---------------------+-------+-------+</span><br><span class="line">| Class               | IoU   | Acc   |</span><br><span class="line">+---------------------+-------+-------+</span><br><span class="line">| wall                | <span class="number">75.88</span> | <span class="number">87.82</span> |</span><br><span class="line">| building            | <span class="number">81.5</span>  | <span class="number">91.98</span> |</span><br><span class="line">| sky                 | <span class="number">93.95</span> | <span class="number">97.18</span> |</span><br><span class="line">| floor               | <span class="number">80.19</span> | <span class="number">90.6</span>  |</span><br><span class="line">| tree                | <span class="number">74.04</span> | <span class="number">87.05</span> |</span><br><span class="line">| ceiling             | <span class="number">82.28</span> | <span class="number">90.21</span> |</span><br><span class="line">| road                | <span class="number">81.98</span> | <span class="number">89.34</span> |</span><br><span class="line">| bed                 | <span class="number">86.94</span> | <span class="number">95.45</span> |</span><br><span class="line">| windowpane          | <span class="number">60.9</span>  | <span class="number">77.16</span> |</span><br><span class="line">| grass               | <span class="number">67.34</span> | <span class="number">83.15</span> |</span><br><span class="line">| cabinet             | <span class="number">59.51</span> | <span class="number">72.18</span> |</span><br><span class="line">| sidewalk            | <span class="number">64.31</span> | <span class="number">80.8</span>  |</span><br><span class="line">| person              | <span class="number">78.34</span> | <span class="number">90.42</span> |</span><br><span class="line">| earth               | <span class="number">37.37</span> | <span class="number">52.52</span> |</span><br><span class="line">| door                | <span class="number">45.28</span> | <span class="number">59.92</span> |</span><br><span class="line">| table               | <span class="number">59.22</span> | <span class="number">76.1</span>  |</span><br><span class="line">| mountain            | <span class="number">57.24</span> | <span class="number">71.79</span> |</span><br><span class="line">| plant               | <span class="number">52.32</span> | <span class="number">63.37</span> |</span><br><span class="line">| curtain             | <span class="number">69.82</span> | <span class="number">83.09</span> |</span><br><span class="line">| chair               | <span class="number">54.96</span> | <span class="number">69.21</span> |</span><br><span class="line">| car                 | <span class="number">83.77</span> | <span class="number">91.67</span> |</span><br><span class="line">| water               | <span class="number">47.62</span> | <span class="number">61.96</span> |</span><br><span class="line">| painting            | <span class="number">72.41</span> | <span class="number">86.03</span> |</span><br><span class="line">| sofa                | <span class="number">60.08</span> | <span class="number">76.23</span> |</span><br><span class="line">| shelf               | <span class="number">43.32</span> | <span class="number">63.98</span> |</span><br><span class="line">| house               | <span class="number">49.56</span> | <span class="number">62.8</span>  |</span><br><span class="line">| sea                 | <span class="number">51.04</span> | <span class="number">70.59</span> |</span><br><span class="line">| mirror              | <span class="number">62.54</span> | <span class="number">69.18</span> |</span><br><span class="line">| rug                 | <span class="number">60.32</span> | <span class="number">67.46</span> |</span><br><span class="line">| field               | <span class="number">31.73</span> | <span class="number">45.61</span> |</span><br><span class="line">| armchair            | <span class="number">37.61</span> | <span class="number">57.33</span> |</span><br><span class="line">| seat                | <span class="number">59.81</span> | <span class="number">79.95</span> |</span><br><span class="line">| fence               | <span class="number">42.07</span> | <span class="number">56.82</span> |</span><br><span class="line">| desk                | <span class="number">49.49</span> | <span class="number">66.97</span> |</span><br><span class="line">| rock                | <span class="number">47.33</span> | <span class="number">73.48</span> |</span><br><span class="line">| wardrobe            | <span class="number">49.47</span> | <span class="number">68.13</span> |</span><br><span class="line">| lamp                | <span class="number">60.04</span> | <span class="number">71.42</span> |</span><br><span class="line">| bathtub             | <span class="number">75.56</span> | <span class="number">81.97</span> |</span><br><span class="line">| railing             | <span class="number">32.42</span> | <span class="number">47.61</span> |</span><br><span class="line">| cushion             | <span class="number">54.0</span>  | <span class="number">68.56</span> |</span><br><span class="line">| base                | <span class="number">30.54</span> | <span class="number">42.64</span> |</span><br><span class="line">| box                 | <span class="number">24.21</span> | <span class="number">29.27</span> |</span><br><span class="line">| column              | <span class="number">43.06</span> | <span class="number">53.74</span> |</span><br><span class="line">| signboard           | <span class="number">36.02</span> | <span class="number">48.73</span> |</span><br><span class="line">| chest of drawers    | <span class="number">39.78</span> | <span class="number">55.26</span> |</span><br><span class="line">| counter             | <span class="number">24.38</span> | <span class="number">34.5</span>  |</span><br><span class="line">| sand                | <span class="number">49.0</span>  | <span class="number">61.36</span> |</span><br><span class="line">| sink                | <span class="number">72.87</span> | <span class="number">79.28</span> |</span><br><span class="line">| skyscraper          | <span class="number">55.51</span> | <span class="number">75.59</span> |</span><br><span class="line">| fireplace           | <span class="number">77.31</span> | <span class="number">86.69</span> |</span><br><span class="line">| refrigerator        | <span class="number">73.98</span> | <span class="number">85.11</span> |</span><br><span class="line">| grandstand          | <span class="number">36.52</span> | <span class="number">66.35</span> |</span><br><span class="line">| path                | <span class="number">25.78</span> | <span class="number">36.24</span> |</span><br><span class="line">| stairs              | <span class="number">30.06</span> | <span class="number">36.85</span> |</span><br><span class="line">| runway              | <span class="number">64.37</span> | <span class="number">86.92</span> |</span><br><span class="line">| case                | <span class="number">36.91</span> | <span class="number">47.58</span> |</span><br><span class="line">| pool table          | <span class="number">91.72</span> | <span class="number">95.42</span> |</span><br><span class="line">| pillow              | <span class="number">54.05</span> | <span class="number">61.68</span> |</span><br><span class="line">| screen door         | <span class="number">57.43</span> | <span class="number">76.01</span> |</span><br><span class="line">| stairway            | <span class="number">28.86</span> | <span class="number">37.94</span> |</span><br><span class="line">| river               | <span class="number">7.04</span>  | <span class="number">18.76</span> |</span><br><span class="line">| bridge              | <span class="number">33.95</span> | <span class="number">43.37</span> |</span><br><span class="line">| bookcase            | <span class="number">43.84</span> | <span class="number">63.89</span> |</span><br><span class="line">| blind               | <span class="number">42.74</span> | <span class="number">47.66</span> |</span><br><span class="line">| coffee table        | <span class="number">56.26</span> | <span class="number">80.03</span> |</span><br><span class="line">| toilet              | <span class="number">84.39</span> | <span class="number">91.65</span> |</span><br><span class="line">| flower              | <span class="number">41.4</span>  | <span class="number">57.24</span> |</span><br><span class="line">| book                | <span class="number">46.29</span> | <span class="number">63.28</span> |</span><br><span class="line">| hill                | <span class="number">6.21</span>  | <span class="number">10.59</span> |</span><br><span class="line">| bench               | <span class="number">50.84</span> | <span class="number">58.05</span> |</span><br><span class="line">| countertop          | <span class="number">47.47</span> | <span class="number">65.09</span> |</span><br><span class="line">| stove               | <span class="number">77.27</span> | <span class="number">82.83</span> |</span><br><span class="line">| palm                | <span class="number">49.09</span> | <span class="number">80.35</span> |</span><br><span class="line">| kitchen island      | <span class="number">35.46</span> | <span class="number">66.41</span> |</span><br><span class="line">| computer            | <span class="number">60.62</span> | <span class="number">69.92</span> |</span><br><span class="line">| swivel chair        | <span class="number">43.89</span> | <span class="number">58.2</span>  |</span><br><span class="line">| boat                | <span class="number">44.83</span> | <span class="number">51.58</span> |</span><br><span class="line">| bar                 | <span class="number">33.78</span> | <span class="number">46.42</span> |</span><br><span class="line">| arcade machine      | <span class="number">42.46</span> | <span class="number">45.03</span> |</span><br><span class="line">| hovel               | <span class="number">27.84</span> | <span class="number">33.58</span> |</span><br><span class="line">| bus                 | <span class="number">85.5</span>  | <span class="number">95.7</span>  |</span><br><span class="line">| towel               | <span class="number">64.81</span> | <span class="number">77.75</span> |</span><br><span class="line">| light               | <span class="number">51.2</span>  | <span class="number">57.51</span> |</span><br><span class="line">| truck               | <span class="number">38.52</span> | <span class="number">49.88</span> |</span><br><span class="line">| tower               | <span class="number">12.59</span> | <span class="number">16.34</span> |</span><br><span class="line">| chandelier          | <span class="number">67.62</span> | <span class="number">83.59</span> |</span><br><span class="line">| awning              | <span class="number">24.46</span> | <span class="number">29.84</span> |</span><br><span class="line">| streetlight         | <span class="number">22.69</span> | <span class="number">27.9</span>  |</span><br><span class="line">| booth               | <span class="number">39.14</span> | <span class="number">43.55</span> |</span><br><span class="line">| television receiver | <span class="number">63.52</span> | <span class="number">76.73</span> |</span><br><span class="line">| airplane            | <span class="number">54.17</span> | <span class="number">68.22</span> |</span><br><span class="line">| dirt track          | <span class="number">9.49</span>  | <span class="number">20.3</span>  |</span><br><span class="line">| apparel             | <span class="number">41.61</span> | <span class="number">58.49</span> |</span><br><span class="line">| pole                | <span class="number">22.26</span> | <span class="number">31.02</span> |</span><br><span class="line">| land                | <span class="number">3.74</span>  | <span class="number">5.16</span>  |</span><br><span class="line">| bannister           | <span class="number">9.37</span>  | <span class="number">11.57</span> |</span><br><span class="line">| escalator           | <span class="number">46.01</span> | <span class="number">61.15</span> |</span><br><span class="line">| ottoman             | <span class="number">46.64</span> | <span class="number">61.2</span>  |</span><br><span class="line">| bottle              | <span class="number">39.34</span> | <span class="number">59.83</span> |</span><br><span class="line">| buffet              | <span class="number">29.0</span>  | <span class="number">32.67</span> |</span><br><span class="line">| poster              | <span class="number">26.59</span> | <span class="number">36.56</span> |</span><br><span class="line">| stage               | <span class="number">19.5</span>  | <span class="number">30.8</span>  |</span><br><span class="line">| van                 | <span class="number">42.08</span> | <span class="number">56.7</span>  |</span><br><span class="line">| ship                | <span class="number">31.27</span> | <span class="number">47.3</span>  |</span><br><span class="line">| fountain            | <span class="number">21.48</span> | <span class="number">22.08</span> |</span><br><span class="line">| conveyer belt       | <span class="number">56.6</span>  | <span class="number">69.16</span> |</span><br><span class="line">| canopy              | <span class="number">6.03</span>  | <span class="number">9.86</span>  |</span><br><span class="line">| washer              | <span class="number">74.13</span> | <span class="number">76.94</span> |</span><br><span class="line">| plaything           | <span class="number">22.11</span> | <span class="number">33.75</span> |</span><br><span class="line">| swimming pool       | <span class="number">57.93</span> | <span class="number">75.19</span> |</span><br><span class="line">| stool               | <span class="number">36.61</span> | <span class="number">54.75</span> |</span><br><span class="line">| barrel              | <span class="number">38.85</span> | <span class="number">65.2</span>  |</span><br><span class="line">| basket              | <span class="number">29.85</span> | <span class="number">39.16</span> |</span><br><span class="line">| waterfall           | <span class="number">61.86</span> | <span class="number">71.23</span> |</span><br><span class="line">| tent                | <span class="number">91.31</span> | <span class="number">98.67</span> |</span><br><span class="line">| bag                 | <span class="number">14.61</span> | <span class="number">18.94</span> |</span><br><span class="line">| minibike            | <span class="number">70.54</span> | <span class="number">82.9</span>  |</span><br><span class="line">| cradle              | <span class="number">69.39</span> | <span class="number">84.86</span> |</span><br><span class="line">| oven                | <span class="number">59.75</span> | <span class="number">75.28</span> |</span><br><span class="line">| ball                | <span class="number">39.28</span> | <span class="number">49.13</span> |</span><br><span class="line">| food                | <span class="number">44.82</span> | <span class="number">55.14</span> |</span><br><span class="line">| step                | <span class="number">15.71</span> | <span class="number">19.1</span>  |</span><br><span class="line">| tank                | <span class="number">56.6</span>  | <span class="number">59.83</span> |</span><br><span class="line">| trade name          | <span class="number">26.23</span> | <span class="number">30.57</span> |</span><br><span class="line">| microwave           | <span class="number">84.77</span> | <span class="number">92.18</span> |</span><br><span class="line">| pot                 | <span class="number">43.37</span> | <span class="number">50.01</span> |</span><br><span class="line">| animal              | <span class="number">50.4</span>  | <span class="number">52.17</span> |</span><br><span class="line">| bicycle             | <span class="number">55.69</span> | <span class="number">76.24</span> |</span><br><span class="line">| lake                | <span class="number">42.86</span> | <span class="number">64.07</span> |</span><br><span class="line">| dishwasher          | <span class="number">62.77</span> | <span class="number">69.52</span> |</span><br><span class="line">| screen              | <span class="number">56.03</span> | <span class="number">74.51</span> |</span><br><span class="line">| blanket             | <span class="number">9.13</span>  | <span class="number">10.76</span> |</span><br><span class="line">| sculpture           | <span class="number">53.36</span> | <span class="number">74.64</span> |</span><br><span class="line">| hood                | <span class="number">58.87</span> | <span class="number">69.21</span> |</span><br><span class="line">| sconce              | <span class="number">43.44</span> | <span class="number">52.69</span> |</span><br><span class="line">| vase                | <span class="number">32.91</span> | <span class="number">42.82</span> |</span><br><span class="line">| traffic light       | <span class="number">28.49</span> | <span class="number">40.94</span> |</span><br><span class="line">| tray                | <span class="number">9.37</span>  | <span class="number">12.05</span> |</span><br><span class="line">| ashcan              | <span class="number">39.81</span> | <span class="number">52.34</span> |</span><br><span class="line">| fan                 | <span class="number">60.07</span> | <span class="number">72.86</span> |</span><br><span class="line">| pier                | <span class="number">54.33</span> | <span class="number">84.51</span> |</span><br><span class="line">| crt screen          | <span class="number">5.25</span>  | <span class="number">16.89</span> |</span><br><span class="line">| plate               | <span class="number">45.0</span>  | <span class="number">57.68</span> |</span><br><span class="line">| monitor             | <span class="number">14.04</span> | <span class="number">19.48</span> |</span><br><span class="line">| bulletin board      | <span class="number">47.86</span> | <span class="number">55.79</span> |</span><br><span class="line">| shower              | <span class="number">0.45</span>  | <span class="number">0.7</span>   |</span><br><span class="line">| radiator            | <span class="number">51.47</span> | <span class="number">61.22</span> |</span><br><span class="line">| glass               | <span class="number">13.14</span> | <span class="number">14.29</span> |</span><br><span class="line">| clock               | <span class="number">31.41</span> | <span class="number">39.73</span> |</span><br><span class="line">| flag                | <span class="number">46.52</span> | <span class="number">50.16</span> |</span><br><span class="line">+---------------------+-------+-------+</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>Semantic Segmentation</tag>
        <tag>Configure Environment</tag>
      </tags>
  </entry>
  <entry>
    <title>跑通Beit V1的语义分割任务</title>
    <url>/2023/08/20/%E8%B7%91%E9%80%9Abeit%20v1%E7%9A%84%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<p><a href="https://github.com/microsoft/unilm/tree/master">Github仓库地址</a></p>
<h3 id="Configure-the-virtual-environment"><a href="#Configure-the-virtual-environment" class="headerlink" title="Configure the virtual environment"></a><strong>Configure the virtual environment</strong></h3><ol>
<li><p>首先，要确保环境为<strong>cuda 11.0</strong>，cuda版本是为之后<code>mmcv-full 1.3.0</code>(包含所有的特性以及丰富的开箱即用的CPU 和<strong>CUDA 算子</strong>)安装做准备。<strong>显卡驱动版本</strong>要支持这个cuda版本（笔者使用腾讯云  V100 GPU云服务器，下图是GPU驱动和cuda版本）<img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308200953127.png" alt="image-20230820040252534"></p>
</li>
<li><p><code>conda create</code>创建一个<code>Python 3.8</code>的虚拟环境，也是为<code>mmcv-full</code>安装做准备</p>
</li>
<li><p><code>pip install torch=1.7.1+cu110</code>：这是最优解，但一开始没有意识到，之后会详细解释。所以我的第二步是进入<code>unlim/beit</code>目录下<code>pip install -r requirements.txt</code></p>
</li>
<li><p>根据<code>unlim/beit/semantic_segmentation/README.md</code>的tutor安装mmcv-full, mmseg, timm, <strong>apex</strong></p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308200953129.png" alt="image-20230820040928616"></p>
</li>
<li><p>但这里安装到apex就出错了，由于pip在之前创建虚拟环境时已经更新至最新版<code>23.2.1</code>, <code>--global-option has already been deprecated</code>，并且显示<code>packaging</code>模块找不到。此时去Nvidia的apex官方仓库，找到了新的命令：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/NVIDIA/apex</span><br><span class="line">cd apex</span><br><span class="line"><span class="comment"># if pip &gt;= 23.1 (ref: https://pip.pypa.io/en/stable/news/#v23-1) which supports multiple `--config-settings` with the same key... </span></span><br><span class="line">pip install -v --disable-pip-version-check --no-cache-<span class="built_in">dir</span> --no-build-isolation --config-settings <span class="string">&quot;--build-option=--cpp_ext&quot;</span> --config-settings <span class="string">&quot;--build-option=--cuda_ext&quot;</span> ./</span><br><span class="line"><span class="comment"># otherwise</span></span><br><span class="line">pip install -v --disable-pip-version-check --no-cache-<span class="built_in">dir</span> --no-build-isolation --<span class="keyword">global</span>-option=<span class="string">&quot;--cpp_ext&quot;</span> --<span class="keyword">global</span>-option=<span class="string">&quot;--cuda_ext&quot;</span></span><br><span class="line"><span class="comment"># only python</span></span><br><span class="line">pip install -v --disable-pip-version-check --no-build-isolation --no-cache-<span class="built_in">dir</span> ./</span><br></pre></td></tr></table></figure>
</li>
<li><p>按照第一个命令，安装后import apex仍然会报错：<code>AttributeError: module &#39;torch.distributed&#39; has no attribute &#39;_all_gather_base&#39;</code>，这是因为torch版本低，但是torch&#x3D;&#x3D;1.7.1是实验要求不可更改，根据github apex<code>issue 1532</code>，要去找branch里的老版本apex –&gt; <code>apex-22.04-dev</code></p>
</li>
<li><p>再安装apex，又比较奇怪，这个版本的apex不支持<code>--config-settings</code>，但使用<code>--global-option</code>对应的命令是可以安装完整版的(cuda+cpp+python)，也不会报错<code>packaging module not found</code></p>
<blockquote>
<p>packaging module not found这个报错在<code>issue 1679</code>中讨论到了，并有<code>pull request 1680</code>，解决方案是在<code>pyproject.toml</code>中修改：</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308200953130.png" alt="image-20230820091717667"></p>
<p>以我之前安装的步骤，这样修改完，再安装apex，会报<code>torch not found</code>，我没有详细论证，但推测和torch 1.7.1有关，这也是接下来要说的</p>
</blockquote>
</li>
<li><p>安装仍然有一个报错，就是关于torch的，因为<code>torch.version.cuda</code>的版本(10.2)和真正使用的cuda版本(11.0)不同，所以需要安装对应cuda版本的torch，我的方法是去<a href="https://download.pytorch.org/whl/torch_stable.html">pytorch官方</a>下载对应的<code>.whl</code>包，并pip安装，之后就可以安装了</p>
</li>
<li><p>接下来就是比较常规的内容：</p>
<ol>
<li>将<code>beit/semantic_segmentation/mmcv_custom</code>添加到环境变量，或者直接复制到对应虚拟环境的<code>site_packages</code>文件夹下，这是因为mmcv_custom调用apex，实现了<code>[&#39;IterBasedRunnerAmp&#39;,&#39;LayerDecayOptimizerConstructor&#39;, &#39;SETR_Resize&#39;, &#39;DistOptimizerHook&#39;, &#39;train_segmentor&#39;]</code>这些训练用的组件</li>
<li>还有就是将<code>mmseg/core/evaluation/metrics.py</code>中的<code>np.float</code>改为<code>float</code>，因为不再支持<code>np.float</code></li>
</ol>
</li>
<li><p>参数：batch size(samples_per_gpu) &#x3D; 4，workers &#x3D; 8，占用显存达到98%</p>
</li>
<li><p>目前仍待解决的问题：(北京时间 8&#x2F;20 9:50 更新)</p>
<ol>
<li><p>直接运行报错<code>Invoked &#39;with amp.scale_loss&#39;, but internal Amp state has not been initialized</code>，根据apex官方给出的文档，，apex只需要三行代码就可以优雅地调用Amp</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308200953131.png" alt="image-20230820093253041"></p>
<p>我们在<code>mmcv_custom/tran_api.py</code>找到了初始化的实现：这里有一个疑惑，为什么<code>cfg.optimizer_config.get(&quot;use_fp16&quot;, False)</code>下还要初始化Amp为混合精度(根据官方文档，<code>opt_level=&quot;O1&quot;</code>即为混合精度，而正因为<code>use_fp16==True</code>，才成为混合精度(乘法用<code>fp16</code>，加法用<code>fp32</code>，混合精度的好处是<strong>可使显存，训练时间大幅减少，但不明显损害训练精度</strong>)，这一点目前没搞懂。</p>
<p>并且，从现有的10000 iterations看，训练过程中震荡很明显，速度很慢(160k iters训练预计使用4 days)，batch size也只能开到4(<code>32G显存 单卡</code>)，并不像真正使用了Amp加速训练，这也引出了第二个问题</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># use apex fp16 optimizer</span></span><br><span class="line"><span class="keyword">if</span> cfg.optimizer_config.get(<span class="string">&quot;type&quot;</span>, <span class="literal">None</span>) <span class="keyword">and</span> cfg.optimizer_config[<span class="string">&quot;type&quot;</span>] == <span class="string">&quot;DistOptimizerHook&quot;</span>:</span><br><span class="line">    <span class="keyword">if</span> cfg.optimizer_config.get(<span class="string">&quot;use_fp16&quot;</span>, <span class="literal">False</span>):</span><br><span class="line">        model, optimizer = apex.amp.initialize(</span><br><span class="line">            model.cuda(), optimizer, opt_level=<span class="string">&quot;O1&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> model.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(m, <span class="string">&quot;fp16_enabled&quot;</span>):</span><br><span class="line">                m.fp16_enabled = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>本实验<strong>是否</strong>真的能通过单GPU训练达到<strong>复现</strong>，因为apex工具包的内容全部关于分布式训练，而<strong>分布式训练核心在于将不同计算核心开不同的进程，增大并行量</strong>，单GPU使用分布式训练（目前我实验使用的方法，因为我感觉代码，尤其是mmcv_custom不容易与分布式训练脱钩）。使用<code>ps aux</code>命令查看进程信息：</p>
<p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308200953132.png" alt="image-20230820094805062"><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308200953133.png" alt="image-20230820094732140"></p>
<blockquote>
<p>可以看出与多进程相关的任务很多，但真正在服务于训练的只有<code>PID 32444</code>的进程，所以分布式训练可能很难真正起作用，因为计算核心只有一个包括即使修复了第一个问题，可以使用混合精度，是否会真正性能提升也是未知数</p>
</blockquote>
</li>
</ol>
</li>
<li><p>解决未使用fp16的问题：(8&#x2F;20 14:28更新)</p>
<ol>
<li><p>之前运行<code>dist_train.sh</code>脚本，总提示</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">RuntimeError: Invoked <span class="string">&#x27;with amp.scale_loss&#x27;</span>, but internal Amp state has not been initialized.  model, optimizer = amp.initialize(model, optimizer, opt_level=...) must be called before with `amp.scale_loss`.</span><br></pre></td></tr></table></figure>

<p>这个报错表示：model和optimizer这两个internal Amp state初始化必须在<code>with amp.scale_loss</code>之前。而<code>with amp.scale_loss</code>的调用在site-packages文件夹下<code>mmseg/apis/train.py</code>的<code>train_segmentor</code>类中。故我们一定要修改它，mmcv_custom(<strong>mmcv_custom提供了一系列在本次实验中适用的api</strong>)中，我们找到了适合的train_segmentor类</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1. 将`optimizer = build_optimizer(model, cfg.optimizer)`放在`if distributed:`之前</span></span><br><span class="line"><span class="string">2. 将初始化Amp state写在optimizer创建和`if distributed:`之间</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># use apex fp16 optimizer</span></span><br><span class="line"><span class="keyword">if</span> cfg.optimizer_config.get(<span class="string">&quot;type&quot;</span>, <span class="literal">None</span>) <span class="keyword">and</span> cfg.optimizer_config[<span class="string">&quot;type&quot;</span>] == <span class="string">&quot;DistOptimizerHook&quot;</span>:</span><br><span class="line">   <span class="keyword">if</span> cfg.optimizer_config.get(<span class="string">&quot;use_fp16&quot;</span>, <span class="literal">False</span>): <span class="comment"># False没懂</span></span><br><span class="line">       model, optimizer = apex.amp.initialize(</span><br><span class="line">           model.cuda(), optimizer, opt_level=<span class="string">&quot;O1&quot;</span>)</span><br><span class="line">       <span class="keyword">for</span> m <span class="keyword">in</span> model.modules():</span><br><span class="line">           <span class="keyword">if</span> <span class="built_in">hasattr</span>(m, <span class="string">&quot;fp16_enabled&quot;</span>):</span><br><span class="line">               m.fp16_enabled = <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>在<code>mmseg/apis/train.py</code>中，我写成了：(因为只要分布式训练，我就用amp)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = build_optimizer(model, cfg.optimizer)</span><br><span class="line"><span class="comment"># put model on gpus</span></span><br><span class="line"><span class="keyword">if</span> distributed:</span><br><span class="line">    find_unused_parameters = cfg.get(<span class="string">&#x27;find_unused_parameters&#x27;</span>, <span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># Sets the `find_unused_parameters` parameter in</span></span><br><span class="line">    <span class="comment"># torch.nn.parallel.DistributedDataParallel</span></span><br><span class="line">    model, optimizer = apex.amp.initialize(model.cuda(), optimizer, opt_level=<span class="string">&quot;O1&quot;</span>)</span><br><span class="line">    model = MMDistributedDataParallel(</span><br><span class="line">        model.cuda(),</span><br><span class="line">        device_ids=[torch.cuda.current_device()],</span><br><span class="line">        broadcast_buffers=<span class="literal">False</span>,</span><br><span class="line">        find_unused_parameters=find_unused_parameters)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>结果：训练时间缩短为一半，但部分loss出现梯度溢出的情况，且显存消耗依然很大，仍有很多需改进的部分</p>
<p>最开始训练时梯度溢出-&gt;推测是: 一开始loss本身过大</p>
<p>网上常见的梯度溢出的可能：</p>
<ol>
<li><code>mask_fill</code>使用-1e9 mask的问题</li>
<li>softmax的溢出</li>
<li>sum函数的溢出</li>
</ol>
<p>为什么我不认为我的是这些常见问题之一：因为这个model的实现没有真正用到fp16，而是一直在用fp32，只有optimizer使用到了fp16，这也是为什么显存始终占用98%，baych size无法提升</p>
</blockquote>
</li>
</ol>
</li>
<li><p>解决无法validation的问题：</p>
<p><code>IterBasedRunnerAmp</code>是现用的runner，按照mmcv_custom的写法，它将被注册到mmcv的<code>registry</code>类当中，但在mmseg的<code>apis/train.py</code>的第110行左右，内容是<code>eval_cfg[&#39;by_epoch&#39;] = cfg.runner[&#39;type&#39;] != &#39;IterBasedRunner&#39;</code>，在这里，<code>IterBaseRunnerAmp</code>并不会被识别为<code>IterBasedRunner</code>，尽管实际上它是。所以，这里我直接改成了<code>eval_cfg[&#39;by_epoch&#39;] = False</code></p>
<blockquote>
<p>validation之后显存不足 -&gt; <strong>new issue</strong> -&gt; 目前先把validation设为<code>160000iterations</code>做一次，打算先花2天跑完 Training，只看最后的validation结果</p>
</blockquote>
</li>
</ol>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a><strong>Results</strong></h3><p><img src="https://lyhkk-1314912494.cos.ap-beijing.myqcloud.com/Project/202308221614170.png" alt="image-20230822161403800"></p>
<blockquote>
<p>差距主要由于batch size(我是4 batchs&#x2F;GPU * 1 GPU，github是2 batchs&#x2F;GPU * 8 GPUs)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">+---------------------+-------+-------+</span><br><span class="line">| Class               | IoU   | Acc   |</span><br><span class="line">+---------------------+-------+-------+</span><br><span class="line">| wall                | <span class="number">79.13</span> | <span class="number">88.61</span> |</span><br><span class="line">| building            | <span class="number">83.17</span> | <span class="number">93.27</span> |</span><br><span class="line">| sky                 | <span class="number">94.67</span> | <span class="number">97.6</span>  |</span><br><span class="line">| floor               | <span class="number">82.83</span> | <span class="number">89.8</span>  |</span><br><span class="line">| tree                | <span class="number">75.83</span> | <span class="number">87.13</span> |</span><br><span class="line">| ceiling             | <span class="number">85.1</span>  | <span class="number">92.23</span> |</span><br><span class="line">| road                | <span class="number">85.48</span> | <span class="number">91.92</span> |</span><br><span class="line">| bed                 | <span class="number">90.11</span> | <span class="number">96.59</span> |</span><br><span class="line">| windowpane          | <span class="number">63.83</span> | <span class="number">77.87</span> |</span><br><span class="line">| grass               | <span class="number">64.76</span> | <span class="number">78.89</span> |</span><br><span class="line">| cabinet             | <span class="number">62.13</span> | <span class="number">73.76</span> |</span><br><span class="line">| sidewalk            | <span class="number">69.88</span> | <span class="number">82.98</span> |</span><br><span class="line">| person              | <span class="number">82.44</span> | <span class="number">92.74</span> |</span><br><span class="line">| earth               | <span class="number">35.82</span> | <span class="number">49.7</span>  |</span><br><span class="line">| door                | <span class="number">51.1</span>  | <span class="number">65.85</span> |</span><br><span class="line">| table               | <span class="number">60.89</span> | <span class="number">74.4</span>  |</span><br><span class="line">| mountain            | <span class="number">59.21</span> | <span class="number">72.16</span> |</span><br><span class="line">| plant               | <span class="number">53.45</span> | <span class="number">64.69</span> |</span><br><span class="line">| curtain             | <span class="number">74.71</span> | <span class="number">87.09</span> |</span><br><span class="line">| chair               | <span class="number">61.9</span>  | <span class="number">74.23</span> |</span><br><span class="line">| car                 | <span class="number">85.93</span> | <span class="number">93.92</span> |</span><br><span class="line">| water               | <span class="number">60.88</span> | <span class="number">75.76</span> |</span><br><span class="line">| painting            | <span class="number">74.08</span> | <span class="number">88.03</span> |</span><br><span class="line">| sofa                | <span class="number">70.52</span> | <span class="number">85.48</span> |</span><br><span class="line">| shelf               | <span class="number">46.76</span> | <span class="number">70.3</span>  |</span><br><span class="line">| house               | <span class="number">48.95</span> | <span class="number">65.93</span> |</span><br><span class="line">| sea                 | <span class="number">66.64</span> | <span class="number">83.46</span> |</span><br><span class="line">| mirror              | <span class="number">70.01</span> | <span class="number">79.44</span> |</span><br><span class="line">| rug                 | <span class="number">68.24</span> | <span class="number">78.04</span> |</span><br><span class="line">| field               | <span class="number">27.0</span>  | <span class="number">45.61</span> |</span><br><span class="line">| armchair            | <span class="number">49.67</span> | <span class="number">68.81</span> |</span><br><span class="line">| seat                | <span class="number">61.57</span> | <span class="number">85.05</span> |</span><br><span class="line">| fence               | <span class="number">50.39</span> | <span class="number">66.54</span> |</span><br><span class="line">| desk                | <span class="number">52.51</span> | <span class="number">73.18</span> |</span><br><span class="line">| rock                | <span class="number">47.69</span> | <span class="number">77.52</span> |</span><br><span class="line">| wardrobe            | <span class="number">52.41</span> | <span class="number">74.16</span> |</span><br><span class="line">| lamp                | <span class="number">64.4</span>  | <span class="number">76.15</span> |</span><br><span class="line">| bathtub             | <span class="number">79.66</span> | <span class="number">84.88</span> |</span><br><span class="line">| railing             | <span class="number">38.76</span> | <span class="number">55.33</span> |</span><br><span class="line">| cushion             | <span class="number">58.95</span> | <span class="number">72.0</span>  |</span><br><span class="line">| base                | <span class="number">35.35</span> | <span class="number">47.07</span> |</span><br><span class="line">| box                 | <span class="number">30.96</span> | <span class="number">39.84</span> |</span><br><span class="line">| column              | <span class="number">48.65</span> | <span class="number">59.64</span> |</span><br><span class="line">| signboard           | <span class="number">39.84</span> | <span class="number">52.53</span> |</span><br><span class="line">| chest of drawers    | <span class="number">42.18</span> | <span class="number">58.8</span>  |</span><br><span class="line">| counter             | <span class="number">25.42</span> | <span class="number">31.12</span> |</span><br><span class="line">| sand                | <span class="number">50.92</span> | <span class="number">78.12</span> |</span><br><span class="line">| sink                | <span class="number">71.8</span>  | <span class="number">81.29</span> |</span><br><span class="line">| skyscraper          | <span class="number">54.8</span>  | <span class="number">75.97</span> |</span><br><span class="line">| fireplace           | <span class="number">71.98</span> | <span class="number">89.09</span> |</span><br><span class="line">| refrigerator        | <span class="number">74.0</span>  | <span class="number">86.95</span> |</span><br><span class="line">| grandstand          | <span class="number">51.51</span> | <span class="number">82.02</span> |</span><br><span class="line">| path                | <span class="number">27.65</span> | <span class="number">38.79</span> |</span><br><span class="line">| stairs              | <span class="number">24.23</span> | <span class="number">32.11</span> |</span><br><span class="line">| runway              | <span class="number">69.69</span> | <span class="number">93.39</span> |</span><br><span class="line">| <span class="keyword">case</span>                | <span class="number">54.84</span> | <span class="number">72.89</span> |</span><br><span class="line">| pool table          | <span class="number">93.59</span> | <span class="number">97.55</span> |</span><br><span class="line">| pillow              | <span class="number">60.04</span> | <span class="number">71.62</span> |</span><br><span class="line">| screen door         | <span class="number">78.78</span> | <span class="number">84.89</span> |</span><br><span class="line">| stairway            | <span class="number">32.42</span> | <span class="number">44.96</span> |</span><br><span class="line">| river               | <span class="number">12.36</span> | <span class="number">26.78</span> |</span><br><span class="line">| bridge              | <span class="number">47.98</span> | <span class="number">56.49</span> |</span><br><span class="line">| bookcase            | <span class="number">42.2</span>  | <span class="number">61.27</span> |</span><br><span class="line">| blind               | <span class="number">44.14</span> | <span class="number">52.09</span> |</span><br><span class="line">| coffee table        | <span class="number">56.83</span> | <span class="number">81.54</span> |</span><br><span class="line">| toilet              | <span class="number">77.65</span> | <span class="number">90.81</span> |</span><br><span class="line">| flower              | <span class="number">40.64</span> | <span class="number">56.71</span> |</span><br><span class="line">| book                | <span class="number">50.39</span> | <span class="number">68.83</span> |</span><br><span class="line">| hill                | <span class="number">8.76</span>  | <span class="number">12.16</span> |</span><br><span class="line">| bench               | <span class="number">44.48</span> | <span class="number">52.77</span> |</span><br><span class="line">| countertop          | <span class="number">53.86</span> | <span class="number">71.89</span> |</span><br><span class="line">| stove               | <span class="number">73.33</span> | <span class="number">85.42</span> |</span><br><span class="line">| palm                | <span class="number">54.7</span>  | <span class="number">81.22</span> |</span><br><span class="line">| kitchen island      | <span class="number">36.69</span> | <span class="number">77.11</span> |</span><br><span class="line">| computer            | <span class="number">76.68</span> | <span class="number">93.44</span> |</span><br><span class="line">| swivel chair        | <span class="number">50.45</span> | <span class="number">70.19</span> |</span><br><span class="line">| boat                | <span class="number">55.72</span> | <span class="number">70.42</span> |</span><br><span class="line">| bar                 | <span class="number">46.97</span> | <span class="number">63.47</span> |</span><br><span class="line">| arcade machine      | <span class="number">77.32</span> | <span class="number">83.73</span> |</span><br><span class="line">| hovel               | <span class="number">44.66</span> | <span class="number">47.13</span> |</span><br><span class="line">| bus                 | <span class="number">88.91</span> | <span class="number">96.9</span>  |</span><br><span class="line">| towel               | <span class="number">66.36</span> | <span class="number">81.26</span> |</span><br><span class="line">| light               | <span class="number">54.84</span> | <span class="number">61.62</span> |</span><br><span class="line">| truck               | <span class="number">38.17</span> | <span class="number">51.35</span> |</span><br><span class="line">| tower               | <span class="number">27.19</span> | <span class="number">45.83</span> |</span><br><span class="line">| chandelier          | <span class="number">67.73</span> | <span class="number">84.32</span> |</span><br><span class="line">| awning              | <span class="number">34.73</span> | <span class="number">40.09</span> |</span><br><span class="line">| streetlight         | <span class="number">29.22</span> | <span class="number">38.42</span> |</span><br><span class="line">| booth               | <span class="number">42.93</span> | <span class="number">48.72</span> |</span><br><span class="line">| television receiver | <span class="number">71.22</span> | <span class="number">81.2</span>  |</span><br><span class="line">| airplane            | <span class="number">62.18</span> | <span class="number">69.12</span> |</span><br><span class="line">| dirt track          | <span class="number">12.35</span> | <span class="number">26.75</span> |</span><br><span class="line">| apparel             | <span class="number">36.96</span> | <span class="number">50.77</span> |</span><br><span class="line">| pole                | <span class="number">22.87</span> | <span class="number">30.81</span> |</span><br><span class="line">| land                | <span class="number">4.12</span>  | <span class="number">7.3</span>   |</span><br><span class="line">| bannister           | <span class="number">15.48</span> | <span class="number">20.49</span> |</span><br><span class="line">| escalator           | <span class="number">58.48</span> | <span class="number">80.13</span> |</span><br><span class="line">| ottoman             | <span class="number">53.12</span> | <span class="number">72.89</span> |</span><br><span class="line">| bottle              | <span class="number">38.93</span> | <span class="number">60.38</span> |</span><br><span class="line">| buffet              | <span class="number">54.7</span>  | <span class="number">64.62</span> |</span><br><span class="line">| poster              | <span class="number">41.08</span> | <span class="number">49.67</span> |</span><br><span class="line">| stage               | <span class="number">15.45</span> | <span class="number">27.8</span>  |</span><br><span class="line">| van                 | <span class="number">44.31</span> | <span class="number">57.28</span> |</span><br><span class="line">| ship                | <span class="number">52.51</span> | <span class="number">77.15</span> |</span><br><span class="line">| fountain            | <span class="number">28.03</span> | <span class="number">29.07</span> |</span><br><span class="line">| conveyer belt       | <span class="number">64.03</span> | <span class="number">93.28</span> |</span><br><span class="line">| canopy              | <span class="number">40.2</span>  | <span class="number">55.69</span> |</span><br><span class="line">| washer              | <span class="number">82.07</span> | <span class="number">86.46</span> |</span><br><span class="line">| plaything           | <span class="number">27.36</span> | <span class="number">41.42</span> |</span><br><span class="line">| swimming pool       | <span class="number">79.88</span> | <span class="number">92.08</span> |</span><br><span class="line">| stool               | <span class="number">46.4</span>  | <span class="number">56.69</span> |</span><br><span class="line">| barrel              | <span class="number">41.77</span> | <span class="number">64.89</span> |</span><br><span class="line">| basket              | <span class="number">35.7</span>  | <span class="number">47.11</span> |</span><br><span class="line">| waterfall           | <span class="number">70.1</span>  | <span class="number">82.53</span> |</span><br><span class="line">| tent                | <span class="number">90.61</span> | <span class="number">99.4</span>  |</span><br><span class="line">| bag                 | <span class="number">16.3</span>  | <span class="number">18.63</span> |</span><br><span class="line">| minibike            | <span class="number">69.95</span> | <span class="number">84.3</span>  |</span><br><span class="line">| cradle              | <span class="number">79.32</span> | <span class="number">97.42</span> |</span><br><span class="line">| oven                | <span class="number">53.71</span> | <span class="number">66.81</span> |</span><br><span class="line">| ball                | <span class="number">51.44</span> | <span class="number">63.86</span> |</span><br><span class="line">| food                | <span class="number">56.48</span> | <span class="number">71.72</span> |</span><br><span class="line">| step                | <span class="number">13.23</span> | <span class="number">14.56</span> |</span><br><span class="line">| tank                | <span class="number">55.92</span> | <span class="number">67.91</span> |</span><br><span class="line">| trade name          | <span class="number">23.79</span> | <span class="number">26.3</span>  |</span><br><span class="line">| microwave           | <span class="number">84.24</span> | <span class="number">93.93</span> |</span><br><span class="line">| pot                 | <span class="number">40.29</span> | <span class="number">45.21</span> |</span><br><span class="line">| animal              | <span class="number">61.39</span> | <span class="number">64.23</span> |</span><br><span class="line">| bicycle             | <span class="number">55.97</span> | <span class="number">73.42</span> |</span><br><span class="line">| lake                | <span class="number">53.18</span> | <span class="number">63.79</span> |</span><br><span class="line">| dishwasher          | <span class="number">60.84</span> | <span class="number">72.38</span> |</span><br><span class="line">| screen              | <span class="number">61.71</span> | <span class="number">87.5</span>  |</span><br><span class="line">| blanket             | <span class="number">16.01</span> | <span class="number">19.36</span> |</span><br><span class="line">| sculpture           | <span class="number">64.79</span> | <span class="number">82.8</span>  |</span><br><span class="line">| hood                | <span class="number">63.25</span> | <span class="number">70.51</span> |</span><br><span class="line">| sconce              | <span class="number">52.4</span>  | <span class="number">63.6</span>  |</span><br><span class="line">| vase                | <span class="number">42.19</span> | <span class="number">56.82</span> |</span><br><span class="line">| traffic light       | <span class="number">30.93</span> | <span class="number">54.64</span> |</span><br><span class="line">| tray                | <span class="number">4.9</span>   | <span class="number">7.15</span>  |</span><br><span class="line">| ashcan              | <span class="number">41.46</span> | <span class="number">55.92</span> |</span><br><span class="line">| fan                 | <span class="number">61.34</span> | <span class="number">76.1</span>  |</span><br><span class="line">| pier                | <span class="number">35.28</span> | <span class="number">43.99</span> |</span><br><span class="line">| crt screen          | <span class="number">17.12</span> | <span class="number">23.36</span> |</span><br><span class="line">| plate               | <span class="number">56.08</span> | <span class="number">73.78</span> |</span><br><span class="line">| monitor             | <span class="number">52.86</span> | <span class="number">64.91</span> |</span><br><span class="line">| bulletin board      | <span class="number">53.05</span> | <span class="number">63.52</span> |</span><br><span class="line">| shower              | <span class="number">0.0</span>   | <span class="number">0.0</span>   |</span><br><span class="line">| radiator            | <span class="number">59.34</span> | <span class="number">65.24</span> |</span><br><span class="line">| glass               | <span class="number">16.52</span> | <span class="number">17.8</span>  |</span><br><span class="line">| clock               | <span class="number">41.97</span> | <span class="number">48.47</span> |</span><br><span class="line">| flag                | <span class="number">60.88</span> | <span class="number">69.39</span> |</span><br><span class="line">+---------------------+-------+-------+</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>Semantic Segmentation</tag>
        <tag>Configure Environment</tag>
        <tag>Coding</tag>
      </tags>
  </entry>
</search>
